# -*- coding: utf-8 -*-
"""NLP_LSTM_Sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z6s_srzPfq9xuKorooQCrv8WYqmpmzpR

# Projek Dicoding
## Belajar Pengembangan Machine Learning
### Natural Language Processing


---
##### Nama: Rangga Wibisana Putra Pamungkas
---
Projek Dicoding dari Belajar Pengembangan Machine Learning ini adalah membuat sebuah model yang akan mengidentifikasi sentiment dari sebuah tweet menggunakan Natural Language Processing Long Short-Term Memory (LSTM) Layer.

# 1. Loading the Dataset
"""

# Menghubungkan Drive dengan Colab
from google.colab import drive
drive.mount('/content/drive')

# Load dataset sebagai dataframe
import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/dataset.csv')

# Melihat sampel teratas dataframe
df.head()

df = df[df['Language']=='en']
df.head()

random_samples = {}
labels = ['positive', 'negative', 'uncertainty', 'litigious']

for label in labels:
    label_data = df[df['Label'] == label]
    random_sample = label_data.sample(n=1500, random_state=42)
    random_samples[label] = random_sample

df_sample = pd.concat(random_samples.values(), ignore_index=True)
df_sample = df_sample.sample(frac=1, random_state=42)
df_sample = df_sample.reset_index(drop=True)
df_sample.head()

df_sample.info()
df_sample.isnull().values.any()

"""# Data Preprocessing"""

# Mengimpor paket yang diperlukan untuk memanipulasi teks dan membuat model.
import re, string, keras, nltk
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.utils.multiclass import unique_labels
from sklearn.metrics import classification_report, accuracy_score, multilabel_confusion_matrix

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from keras.callbacks import ReduceLROnPlateau, Callback

stop_words = set(stopwords.words('english')) | set(string.punctuation)
stop_words.update({'Ã', '±', 'ã', '¼', 'â', '»', '§'})

def clean_text(string):
  result = string.replace('\r\n', ' ').lower()  # Convert to lowercase and remove newlines
  result = re.sub(r'<[^>]+>\s*', ' ', result)  # Remove HTML tags
  result = re.sub(r"(?:\@|https?\://)\S+", "", result)  # Remove URLs
  # Remove non-ASCII characters
  result = re.sub(r'[^\x00-\x7f]', '', result)
  result = re.split('#(?!(?:hashtag)\b)[\w-]+(?=(?:\s+#[\w-]+)*\s*$)', result)
  result = " ".join(word.strip() for word in result)
  result = " ".join(word.strip() for word in re.split('#|_', result))
  result = re.sub('[^a-zA-Z\s]', ' ', result)  # Remove non-letters and non-spaces
  result = re.sub(r'\s+[a-zA-Z]\s+', ' ', result)
  result = re.sub(r'\s+', ' ', result)  # Collapse consecutive spaces
  result = ' '.join(word for word in result.split() if word not in stop_words)  # Remove stop words
  return result

# Apply cleaning and stop word removal to the 'review' column
df_sample['Text'] = df_sample['Text'].apply(lambda x: clean_text(x))
df_sample.head()

"""# Encoding Labels and Making Train-Test Splits"""

category = pd.get_dummies(df_sample.Label)
df_baru = pd.concat([df_sample.Text , category], axis=1)
df_baru.head()

text = df_baru['Text'].values
labels = df_baru[['litigious', 'negative', 'positive', 'uncertainty']].values

# Membagi kumpulan data menjadi train dan test menggunakan train_test_split dengan 20% untuk test.
text_train, text_test, labels_train, labels_test = train_test_split(
    text, labels,
    test_size = 0.2,
    random_state=42,
    shuffle=True,
    stratify=labels
    )

def get_max_length():
    review_length = []
    for review in text_train:
        review_length.append(len(review))

    return int(np.ceil(np.mean(review_length)))

# Melakukan tokenisasi dan mengkonversi sampel menjadi sequence
tokenizer = Tokenizer(num_words=5000, lower=False, oov_token='')
tokenizer.fit_on_texts(text_train)
tokenizer.fit_on_texts(text_test)

max_length = get_max_length()

# convert train dataset to sequence and pad sequences
train_sequences = tokenizer.texts_to_sequences(text_train)
train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length, truncating='post')

# convert Test dataset to sequence and pad sequences
test_sequences = tokenizer.texts_to_sequences(text_test)
test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length, truncating='post')

vocab_size = len(tokenizer.word_index) + 1 #index start from 0
print(vocab_size)
print(max_length)

"""# Building the Model"""

# model initialization
model = keras.Sequential([
    keras.layers.Embedding(vocab_size, 300, input_length=max_length),
    keras.layers.Bidirectional(keras.layers.LSTM(64)),
    keras.layers.Dropout(0.4),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.4),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.4),
    keras.layers.Flatten(),
    keras.layers.Dense(4, activation='softmax')
])

# compile model
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# model summary
model.summary()

# Callback untuk mengurangi learning rate jika tidak ada peningkatan pada akurasi
reduce_lr = ReduceLROnPlateau(
    monitor="accuracy",
    patience=2,
    verbose=1,
    factor=0.5,
    min_lr=0.00001,
)


# Custom callback untuk menghentikan pelatihan jika akurasi mencapai lebih dari 90%
class MyEarlyStop(Callback):
    def on_epoch_end(self, epoch, logs=None):
        if (logs.get('accuracy') >= 0.9 and logs.get('val_accuracy')>=0.9):
            print(
                '\nFor Epoch',
                epoch+1,
                '\nAccuracy has reached = %2.2f%%' % (logs['accuracy'] * 100),
                ', Val Accuracy = %2.2f%%' % (logs['val_accuracy'] * 100),
                ', training has been stopped.'
            )
            self.model.stop_training = True

"""# Model Training and Evaluation"""

# Melakukan pelatihan pada model
history = model.fit(train_padded, labels_train,
                    epochs=30, verbose=1, batch_size = 128,
                    validation_data=(test_padded, labels_test),
                    callbacks=[reduce_lr, MyEarlyStop()])

# Menampilkan plot accuracy dan loss
fig, ax = plt.subplots(1,2)
train_acc = history.history['accuracy']
train_loss = history.history['loss']
val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']
fig.set_size_inches(20,10)

# Plot accuracy dan loss dari training
ax[0].plot(train_acc , 'go-' , label = 'Training Accuracy')
ax[0].plot(val_acc , 'ro-' , label = 'Testing Accuracy')
ax[0].set_title('Training & Testing Accuracy')
ax[0].legend()
ax[0].set_xlabel("Epochs")
ax[0].set_ylabel("Accuracy")

# Plot accuracy dan loss dari testing
ax[1].plot(train_loss , 'go-' , label = 'Training Loss')
ax[1].plot(val_loss , 'ro-' , label = 'Testing Loss')
ax[1].set_title('Training & Testing Loss')
ax[1].legend()
ax[1].set_xlabel("Epochs")
ax[1].set_ylabel("Loss")
plt.show()

# Melakukan prediksi pada data testing
pred = model.predict(test_padded)
pred[:5]

# Ubah prediksi menjadi 1
pred_mod = np.zeros_like(pred)
for i in range(len(pred)):
    pred_mod[i] = np.zeros(len(pred[i]))
    pred_mod[i][np.argmax(pred[i])] = 1

# Print hasil
print(pred_mod)

# Menampilkan Classification Report dari model
print(classification_report(labels_test, pred_mod, target_names = ['litigious', 'negative', 'positive', 'uncertainty']))